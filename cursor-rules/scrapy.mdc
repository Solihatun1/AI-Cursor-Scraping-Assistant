---
description: 
globs: 
alwaysApply: true
---
---
description: This rule provides comprehensive best practices for Scrapy development, including code organization, performance, security, testing, and common pitfalls to avoid. It aims to guide developers in building robust, efficient, and maintainable web scraping applications with Scrapy.
globs: **/*.py
---
# Scrapy Best Practices

This document outlines the recommended best practices for developing Scrapy web scraping applications. Following these guidelines will help you create robust, efficient, secure, and maintainable scrapers.

Before start developing a Scrapy spider, follow the [prerequisites.mdc](mdc:.cursor/rules/prerequisites.mdc) rule.

Only after reading the [prerequisites.mdc](mdc:.cursor/rules/prerequisites.mdc) files and executing the steps included in it, you can start building the Scrapy spider.

When asked to build a Scrapy spider, read the [scraper-models.mdc](mdc:.cursor/rules/scraper-models.mdc) rule for understanding the type of scraper to create.

Never change the starting URL of the scraper, even in case of errors during the process, but always use the one given by the user.

## 1. Code Organization and Structure

### 1.1. Directory Structure

-   **Project Root:** Contains `scrapy.cfg`, project directory, and any `README.md`, `LICENSE`, or other project-level files.
-   **Project Directory (e.g., `my_project`):**
    -   `__init__.py`:  Marks the directory as a Python package.
    -   `items.py`: Defines the data structures (Scrapy Items) for the scraped data.
    -   `middlewares.py`:  Contains the Scrapy middleware components, used for request/response processing.
    -   `pipelines.py`:  Defines the data processing pipelines, used for cleaning, validating, and storing the scraped data.
    -   `settings.py`:  Configures the Scrapy project, including settings for pipelines, middleware, concurrency, etc.
    -   `spiders/`:
        -   `__init__.py`:  Marks the directory as a Python package.
        -   `my_spider.py`: Contains the spider definitions (Scrapy Spiders) responsible for crawling and scraping data.

Example:


my_project/
├── scrapy.cfg
├── my_project/
│   ├── __init__.py
│   ├── items.py
│   ├── middlewares.py
│   ├── pipelines.py
│   ├── settings.py
│   └── spiders/
│       ├── __init__.py
│       └── my_spider.py
└── README.md


### 1.2. File Naming Conventions

-   **Spider Files:** `spider_name.py` (e.g., `product_spider.py`, `news_spider.py`)
-   **Item Files:** `items.py` (standard naming)
-   **Middleware Files:** `middlewares.py` (standard naming)
-   **Pipeline Files:** `pipelines.py` (standard naming)
-   **Settings Files:** `settings.py` (standard naming)

### 1.3. Module Organization

-   **Small Projects:** All spiders can reside in the `spiders/` directory.
-   **Large Projects:** Consider organizing spiders into subdirectories based on the target website or data type (e.g., `spiders/news/`, `spiders/ecommerce/`).
-   **Custom Modules:** Create custom modules (e.g., `utils/`, `lib/`) for reusable code, helper functions, and custom classes.

### 1.4. Component Architecture

-   **Spiders:** Focus on crawling and extracting raw data.
-   **Items:** Define the structure of the scraped data.
-   **Pipelines:** Handle data cleaning, validation, transformation, and storage.
-   **Middleware:** Manage request/response processing, error handling, and proxy management.

### 1.5. Code Splitting

-   **Separate Concerns:**  Keep spiders lean and focused on crawling logic. Move data processing and storage to pipelines.
-   **Reusable Components:**  Extract common functionality (e.g., custom item loaders, helper functions) into separate modules or classes.
-   **Configuration:**  Use `settings.py` to manage project-wide configuration and avoid hardcoding values in spiders.
-   **Modular Middleware:** Create small, focused middleware components for specific tasks (e.g., user agent rotation, request retries).
